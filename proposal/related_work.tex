\section{Related Work}
\label{sec:related_work}

In this section, I survey related works in the structure of mechanisms providing new memory protection and better isolations between mutually untrusted components; mitigations of code reuse attacks and automated bug finding systems that identify software defects leading to in-process memory abuse.

\subsection{Memory Protection and Isolation}
Several memory protection mechanisms were proposed before. Overshadow~\cite{chen2008overshadow} uses virtualization to render encrypted views of application memory to untrusted OS, and in turn, protects application data. Mondrian~\cite{mondrian} is a hardware-level memory protection scheme that enables permission control at word-granularity and allows memory sharing among multiple protection domains. Another scheme~\cite{suh2003efficient} provides memory encryption and integrity verification for secure processors.
Recently, protecting cryptographic keys in memory became a popular research topic. Proposed solutions range from minimizing key exposure in memory~\cite{harrison2007protecting,heartbleedpatch,securestring}, to avoiding key presence in the RAM by confining key operations to CPUs~\cite{muller2011tresor,guan2014copker}, GPUs~\cite{vasiliadis2014pixelvault}, and hardware transactional memory~\cite{Mimosa15}.

There are numerous research proposing isolation for untrusted components, ranging from libraries in user-space programs to drivers in the OS. Software Fault Isolation (SFI~\cite{sfi}) and its variants~\cite{fastsfi,erlingsson2006xfi} establish strict boundaries in memory space to isolate potentially faulty modules and therefore contain the impact resulted from the crashes or malfunctions of such modules. SFI has also been extended to build sandboxes for untrusted plugins and libraries on both x86 ~\cite{vx32,yee2009native} and ARM~\cite{naclarm,zhou2014armlock}. Extending module isolation into kernel-space, some previous works~\cite{erlingsson2006xfi,strackx2012fides} contain faulty drivers as well as user-space modules. 
Separating program components into different processes has long been advocated as a practical approach to achieve privilege and memory separation~\cite{kilpatrick2003privman,provos2003preventing,brumley2004privtrans,wireframe}. However, process separation can cause high overhead, particularly when separated components frequently interact. Wedge enables thread-level memory isolation~\cite{wedge}. While incurring slightly lower overhead than process-level isolation, it still suffers from the fixed granularity and require major software changes to be adopted.
Some recent works proposed fine-grained and flexible application sandbox~\cite{watson2010capsicum,belay2012dune} and compartmentalization~\cite{watson2015cheri} frameworks. These works mainly aim at mitigating memory-related exploitations by reducing the capabilities and privileges for untrusted or vulnerable code. 

A number of systems were proposed for securely executing sensitive code or performing privileged tasks. Flicker ~\cite{mccune2008flicker} allows for trusted code execution in full isolation to OS or even BIOS and provides remote attestation. TrustVisor~\cite{mccune2010trustvisor} improves on performance and granularity with a special-purpose hypervisor. SeCage~\cite{secage} runs sensitive code in a secure VM. SICE~\cite{azab2011sice} protects sensitive workloads purely at the hardware level and supports current execution on multicore platforms. SGX~\cite{sgx}, a recent feature in Intel CPUs, allows user-space programs to create so-called enclaves where sensitive code can run securely but has little access to system resources or application context.

\subsection{Mitigations of Code Reuse Attacks}
Over the years, there has been an ongoing race between code reuse attacks (e.g, ROP) and corresponding defense countermeasures. Such code reuse attacks keep evolving into new forms with more complex attack steps (e.g., Blind-ROP~\cite{brop}, JIT-ROP \cite{jitrop}). To defend against them, three categories of countermeasures (e.g., ASLR, XOM, CFI) have been proposed from different perspectives.

ASLR is a practical and popular defense deployed in modern operating systems to thwart code reuse attacks~\cite{aslr}. It randomizes the memory address and makes the locations of ROP gadgets unpredictable. However, the de-facto ASLR only randomizes the base address of code pages. It becomes ineffective when facing recent memory-disclosure-based code reuse attacks~\cite{brop,jitrop}. Such attack explores the address space on-the-fly to find ROP gadgets via a memory disclosure vulnerability. Although fine-grained ASLR increases the entropy of randomization, such as compile-time code randomization~\cite{bhatkar2005efficient} and load-time randomization~\cite{davi2013gadge,ilr,aslp,binstir,ccr} the memory disclosure attack is not directly addressed, since code pages can still be read by attackers ~\cite{jitrop}. Runtime randomization~\cite{isomeron,remix,timelyrandom} is thus proposed to introduce more uncertainty into the program's address space.

To address the memory disclosure attack, researchers proposed execute-only but non-readable (R $\otimes$ X) memory pages to hinder the possibility of locating reusable code (or ROP gadgets). However, one fundamental challenge to achieve this defense is that it is non-trivial to identify and separate legitimate data read operations in code pages. 
When source code is available, existing works like Readactor~\cite{readactor,readactorpluplu} and LR2~\cite{lr2} rely on compilers to separate data reads from code pages and then enforcing XOM via either hardware-based virtualization or software-based address masking. On the other hand, for COTS binaries, which are more common in the real-world scenario, XnR~\cite{xnr} blocks direct memory disclosure by modifying the page fault handler in operating systems to check whether a memory read is inside a code or data region of a process. However, it cannot handle embedded data mixed in code region. HideM~\cite{hidem} utilizes split-TLB features in AMD processors to distinguish direct code and data access to different physical pages to prevent reading code. Unfortunately, recent processors no longer support split-TLB.


Enforcing CFI is a general defense against code reuse attacks. Proposed a decade ago by PaX Team and Abadi et al.~\cite{paxcfi,abadi2005control}, CFI has been tuned by researchers over the years~\cite{ifccvtv,rockjit,picfi,ccfi,opaquecfi,rapteampax}, from its early form coarse-grained CFI to its current mature appearance as fine-grained CFI. The fundamental difference is that a coarse-grained CFI allows forward edges in the control flow graph (CFG) to point at any node in the graph and back-ward edges to return to any call preceded destination, whilst a fine-grained CFI has a more precise set of destinations for both forward and backward edges. bin-CFI \cite{bincfi} and CCFIR \cite{ccfir} enforce the coarse-grained CFI policy on Linux and windows COTS binaries respectively. Unfortunately, enforcing a fine-grained CFI requires a more precise CFG to be built as the ground truth, which is difficult to obtain in practice based on static analysis, even when source code is available. In addition, researchers found that it is still possible to launch code reuse attacks when fine-grained CFI solution is in place due to the difficulty of extracting a perfect CFG in practice~\cite{outofcontrol,stitchinggadget,controlflowbending,controljujutsu}.

\subsection{Automated Bug Finding Tools}
Symbolic execution, a systematic program testing methodintroduced in 1970s~\cite{King76, Howden77}, has attracted new attentions dueto the advances in the satisfiability modulo theory~\cite{GaneshD07, MouraDS07,MouraB11}.However, classic symbolic execution suffers from the expensivecomputation cost as well as the state explosion problem:it solves the feasibility of both conditional branches whenreaching a branching pivot point, and generates unbiasednew states accordingly. To tackle these issues Sen proposedconcolic execution~\cite{Sen07a}, a variant of symbolic execution,by combining the concrete input generation from symbolicexecution and the fast execution of random testing. Concolicexecution increases the coverage of random testing~\cite{sage,GodefroidKS05}while also scaling to large applications, hence has been studiedin various frameworks~\cite{SenA06,SenMA05,BurnimS08,s2e}. It also plays an criticalrole in the automated vulnerability detection and exploitation,where the concolic component generates security-related inputby incorporating extra safety predicates~\cite{ChaARB12,AEG}. However,concolic execution runs in virtual machines, and the executionoverhead challenges its application to practical softwarewith rich environmental interactions. In contrast, fuzz testingexecutes in native speed but it lacks transparent information 
about the code, whereas mixing these two techniques becomesa promising schema.

Majundar et al.~\cite{majumdar2007hybrid} introduced the idea of Hybrid ConcolicTesting around one decade ago. This idea offsets the deficiencyof both random testing and concolic execution. Specifically,their approach interleaves random testing and concolic executionto deeply explore a wide space of program state. Subsequentdevelopment reinforces hybrid testing by replacing randomtesting with guided fuzzing~\cite{pak2012hybrid}. This approach could rapidlyincrease the code coverage by contributing more high-qualityseeds to concolic execution. Recently, Driller~\cite{driller} engineersthe state-of-art hybrid testing system. It more coherently combinesfuzzing and concolic execution and can seamlessly testa wide range of software systems. Despite of the remarkableadvancement in hybrid testing, Driller still suffers from theunsound vulnerability detection. DigFuzz~\cite{DigFuzz} is a more recentwork that tries to better coordinate the fuzzing and concolicexecution components of hybrid testing. Based on a MonteCarlo algorithm, DigFuzz predicts the difficulty for a fuzzerto explore a path and prioritize exploring seeds with higherdifficulty score. Moreover, motivated by the increasing securitydemands in software systems, researchers have been reasoningthe performance of hybrid testing. One common intuition isthat hybrid testing is largely restricted by the slowness ofconcolic executor. In particular, QSYM~\cite{qsyminsu} implements asymbolic executor that tailors the heavy but unnecessary computationsinvolved in symbolic emulation and constraint solving.It can often lead to times of acceleration.

Many recent works focus on improving the capability ofcode exploration in fuzzing. CollAFL~\cite{collafl} aims to reducehash collision to avoid false negatives when exploring newcode. To generate smarter seeds, ProFuzzer~\cite{profuzzer} infers theformat of inputs and use it as guidance in later stage fuzzing.Along the line of smart fuzzing, Angora~\cite{angora} assumes a blackboxfunction for each program condition and uses gradientdescent to search for satisfying input bytes. This method is
later improved by NEUZZ~\cite{neuzz} using a smooth surrogatefunction to approximate the behavior of the tested program.

%Another lines of work aimed to guide symbolic execution%and fuzzing to explore given code locations for various purposes.%Katch~\cite{MarinescuC13} targets testing software patches based on%symbolic execution. It selects from regression suite an existing%input which triggers code close to the patch location as the%seed for symbolic execution, and then augments three specific%heuristics to guide symbolic executor to reach the patch code%faster. Sharing a similar goal, AFLGO~\cite{aflgo} instruments the%tested program to calculate code distance to the target function%as fuzzing progress and its scheduling heuristics favors seeds%with shorter distance. Christakis et al.~\cite{christakis} implements a%path pruning algorithm for dynamic symbolic execution by%checking if a state is on a path with already verified properties.

